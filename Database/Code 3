!pip install tensorflow
!pip install transformers
nltk.download()
import pandas as pd
import numpy as np
import string
import nltk
from tensorflow.keras.layers import TextVectorization, Embedding, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer  # Optional for TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer  # Optional for TF-IDF
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer


# Load historical data (replace with your data loading logic)

data = pd.read_csv("GOOGLE_test.csv", encoding="utf-8", delimiter=";")
date_col = "Date"  # Column containing the date
price_col = "Close"  # Column containing the closing price
news_col = "News_Article"  # Column containing the news text (optional)


# Prepare data
data.iloc[:, 4] = data.iloc[:, 2].shift(-1)  # Shift price for prediction
data.dropna(inplace=True)  # Remove rows with missing values

# Split data into training and testing sets
train_size = int(len(data) * 0.8)
train_data, test_data = data[:train_size], data[train_size:]

# Text Preprocessing (if using news articles)
def preprocess_text(text):
  # Lowercase text
  text = text.lower()

  # Remove punctuation
  text = text.translate(str.maketrans('', '', string.punctuation))

  # Remove stopwords
  stop_words = stopwords.words('english')
  text = ' '.join([word for word in text.split() if word not in stop_words])

  return text


# Apply preprocessing to each headline
train_news = train_data[news_col].apply(preprocess_text)
test_news = test_data[news_col].apply(preprocess_text)

# Text Vectorization (if using news articles)
max_vocab_size = 10000  # Adjust based on your data
vectorizer = TextVectorization(max_tokens=max_vocab_size)
vectorizer.fit_on_texts(train_news.tolist() + test_news.tolist())

train_news_sequences = vectorizer(train_news.tolist())
test_news_sequences = vectorizer(test_news.tolist())

# Combining features (consider appropriate concatenation based on your data)
train_features = {
    "news": train_news_sequences,
    "price": train_data[price_col].values.reshape(-1, 1)  # Reshape for 2D array
}
test_features = {
    "news": test_news_sequences,
    "price": test_data[price_col].values.reshape(-1, 1)
}

# Define look-back window
look_back = 5  # Number of past days (including news) to consider for prediction

def create_sequences(features, window_size):
  sequences = []
  for i in range(len(features["price"]) - window_size):
    news_sequence = features["news"][i:i+window_size]
    price_sequence = features["price"][i:i+window_size]
    # feature_sequence = features[1][i:i+window_size]  # Feature sequence (keywords or topics)
    sequence = np.concatenate((news_sequence, price_sequence), axis=1)  # Concatenate news and price
    sequences.append(sequence)
  return sequences

train_sequences = create_sequences(train_features.copy(), look_back)
test_sequences = create_sequences(test_features.copy(), look_back)

# Convert sequences to numpy arrays
train_sequences = np.array(train_sequences)
test_sequences = np.array(test_sequences)

# Build Transformer model
model = Sequential()
model.add(Embedding(max_vocab_size, embedding_dim=128, input_shape=(look_back, None)))  # Embedding for news
model.add(Transformer(num_layers=2, units=64, head_size=8))  # Adjust hyperparameters as needed
model.add(Dense(units=1))  # Output layer for predicted price

# Compile model
model.compile(loss="mse", optimizer="adam")

# Train the model
model.fit(train_sequences, train_data["Future_Price"], epochs=50, batch_size=32)

# Make predictions on test data
predicted_prices = model.predict(test_sequences)
